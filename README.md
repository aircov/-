# recmd_project
## data_nlp/KerasSentimentAnalysis

Keras情感分析（Sentiment Analysis）实战---自然语言处理技术

情感分析任务其实是个**分类任务**，给模型输入一句话，让它判断这句话的情感是积极的，消极的，还是中性的。例子如下：
 输入：的确是专业，用心做，出品方面都给好评。
 输出：2
 输出可以是[0,1,2]其中一个，0表示情感消极，1表示情感中性，2表示情感积极。

#### 情感分析算法简介

分类任务的算法，想必大家都很熟悉：SVM，Logistic，Tree等。可是对于文本分类来说，最重要的是如何将一句话的映射到向量空间，同时保持其语义特征。所以文本的**向量化表示**是最最重要的一个环节。而文本的向量化就是涉及到**Word Embedding**技术和**深度学习(Deep Learning)**技术。

**Word Embedding**指的是把文本转换成计算机能处理的向量，而其中难点的是：将文本向量化时如何保持句子原有的语义。早期word embedding使用的是Bag of Words，TF-IDF等，这些算法有个共同的特点：就是没有考虑语序以及上下文关系。而近几年发展出来的**Word2Vector ，Glove**等考虑到了文本的上下文关系。今年NLP领域大放异彩的**BERT**就是在文本向量化上做出了重大的突破。

人工特征的挖掘是个极为费脑费时的过程，**深度学习**模型可以将特征工程自动化，通过神经网络自动做特征的表示学习。在NLP领域中,**RNN**(LSTM,GRU)，**CNN**，**Transformer**等各路深度学习模型各显神通，凭借他们强大的特征表示能力，在很多任务中都吊打人工特征（吹得  有些夸张了，没收住）。不过人工特征有时还是很重要的。



本次的项目实战的总体架构可分为两个步骤：
（1）采用Word2Vector技术去训练词向量；
（2）采用BiLSTM去做特征的表示学习。

#### 训练词向量

将所有的评论文本数据用来训练词向量，这里使用的gensim中的Word2Vec,原理是的Skip-gram。这里对词向量的原理不多介绍，总之，这一步将一个词映射成一个100维的向量，并且考虑到了上下文的语义。这里直接将上一部得到的句子列表传给train_word2vec函数就可以了，同时需要定义一个词向量文件保存路径。模型保存后，以后使用就不需要再次训练，直接加载保存好的模型就可以啦。

#### 数据预处理

这里定义了一些数据处理和变换方法。

###### 获取词向量矩阵和词典

```undefined
w2id,embedding_weights = generate_id2wec(model)
```

这一步主要是为了拿到传给后续情感分析模型的词典（w2id）和词向量矩阵embedding_weights，
 **w2id格式**如下：{
 ...
 '一两天': 454,
 '一两年': 455,
 '一两次': 456,
 '一个': 457,
 '一个个': 458,
 '一个劲': 459,
 ...
 '不一会': 984,
 '不上': 985,
 '不下': 986,
 '不严': 987,
 '不为过': 988,
 '不久': 989,
 }

embedding_weights格式如下:

[[ 0.        ,  0.        ,  0.        , ...,  0.        ,

0.        ,  0.        ],

[-1.1513499 , -0.00520114,  1.65645397, ...,  0.50586915,

-0.03466858,  0.84113288],

[ 0.01824509, -0.23613754, -0.47191045, ..., -0.16491373,

-0.25222906, -0.00384654],

...,

[ 0.10879639,  0.05459598, -0.02946772, ..., -0.17389177,

0.10144144,  0.21539673]]

这个矩阵保存了上面通过Word2Vector方法训练的词向量，每个词通过其在词典（w2id）中的index索引到对应得词向量，此矩阵将作为参数传给后续的情感分析模型。

将数据变换成模型能够处理的格式。
 原始数据格式如下：
 sen :不错，品种齐全，上菜很快，味道也不错
 label ：2

执行上面代码后句子数据变成如下格式：
 输入：[0，0，0......,31,43,12,4,65,12,233,11,1391,131,4923,1233]
 输出：[0，0，1]



## data_nlp/TextClassification

文本分类一般可以分为二分类、多分类、多标签分类三种情况,二分类是指将一组文本分成两个类(0或1),比较常见的应用如垃圾邮件分类、电商网站的用户评价数据的正负面分类等,多分类是指将文本分成若干个类中的某一个类,比如说门户网站新闻可以归属到不同的栏目中(如政治、体育、社会、科技、金融等栏目)去。多标签分类指的是可以将文本分成若干个类中的多个类,比如一篇文章里即描写政治又描写金融等内容，那么这篇文章可能会别贴上政治和金融两个标签。尝试使用Python和sklearn来实现一下文本的多分类实战开发。

```
我们将使用如下四种模型:
Logistic Regression(逻辑回归)
(Multinomial) Naive Bayes(多项式朴素贝叶斯)
Linear Support Vector Machine(线性支持向量机)
Random Forest(随机森林)
```

各种模型准确率：

```
model_name
LinearSVC                 0.855110
LogisticRegression        0.839418
MultinomialNB             0.773005
RandomForestClassifier    0.546248
Name: accuracy, dtype: float64
```

## HotWords

爬虫代码，自动爬取百度热搜，微博热搜，存储在mysql；

mysql数据增量/全量保存到elasticsearch；

save_to_redis：将mysql中query词和热度值保存在redis，hash结构，便于在搜索联想的时候按照heat排序。

## Search

搜索接口：<http://47.102.99.199:5000/api/search/content/?keyword=%E7%96%AB%E6%83%85&page=1&limit=10>

搜索联想接口：<http://47.102.99.199:5000/>

评论敏感词过滤接口：http://47.102.99.199:5000/api/sensitive

Post---body {"txt":"国家主席习近平在中国青岛主持上海合作组织成员国元首理事会第十八次会议。王八蛋然后就是fuck dog 跟随着egg 主人公怒哀乐情节中。法.轮#功 难过就手机卡复制器 ，一个贱人 !！去看电影。小姐姐真漂亮，像个大王八,大王八，你妈的！cao大鸡儿蛋大刀肉。"}

~~~json
{
    "txt": "国家主席习近平在中国青岛主持上海合作组织成员国元首理事会第十八次会议。王八蛋然后就是fuck dog 跟随着egg 主人公怒哀乐情节中。法.轮#功 难过就手机卡复制器 ，一个贱人 !！去看电影。小姐姐真漂亮，像个大王八,大王八，你妈的！cao大鸡儿蛋大刀肉。",
    "txtLength": 129,
    "regularResult": "非广告文本",
    "ifContainSensitiveWord": true,
    "sensitiveWordCount": 12,
    "sensitiveWordList": "[政治:习近平; 其他:王八; 其他:fuck; 暴恐:法.轮#功; 暴恐:轮#功; 社会:手机卡复制器; 色情:贱人; 其他:王八,; 其他:王八，; 其他:你妈的！; 色情:cao; 色情:鸡儿]",
    "score": 6,
    "grade": "掩码",
    "code": 4002,
    "total_time": 0.0019259452819824219,
    "txtReplace": "国家主席***在中国青岛主持上海合作组织成员国元首理事会第十八次会议。**蛋然后就是**** dog 跟随着egg 主人公怒哀乐情节中。***** 难过就****** ，一个** !！去看电影。小姐姐真漂亮，像个大**,大**，*******大**蛋大刀肉。"
}
~~~

