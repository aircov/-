{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文评论情感分析(keras+rnn）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "#KeyedVectors实现实体（单词、文档、图片都可以）和向量之间的映射，实体都用string id表示\n",
    "#有时候运行代码时会有很多warning输出，如提醒新版本之类的，如果不想乱糟糟的输出可以这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练词向量\n",
    "# 使用gensim加载预训练中文分词embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('data/sgns.zhihu.bigram',\n",
    "                                          binary=False)\n",
    "# 词向量模型\n",
    "# 在这个词向量模型里，每一个词是一个索引，对应的是一个长度为300的向量，我们今天需要构建的LSTM神经网络模型并不能直接处理汉字文本，\n",
    "# 需要先进行分次并把词汇转换为词向量，步骤请参考：\n",
    "# 0.原始文本：我喜欢文学\n",
    "# 1.分词：我，喜欢，文学\n",
    "# 2.Tokenize(索引化)：[2,345，4564]\n",
    "# 3.Embedding(词向量化)：用一个300维的词向量，上面的tokens成为一个[3，300]的矩阵\n",
    "# 4.RNN:1DCONV,GRU,LSTM等\n",
    "# 5.经过激活函数输出分类：如sigmoid输出在0到1间\n",
    "# 由此可见每一个词都对应一个长度为300的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的长度为300\n",
      "[-2.603470e-01  3.677500e-01 -2.379650e-01  5.301700e-02 -3.628220e-01\n",
      " -3.212010e-01 -1.903330e-01  1.587220e-01 -7.156200e-02 -4.625400e-02\n",
      " -1.137860e-01  3.515600e-01 -6.408200e-02 -2.184840e-01  3.286950e-01\n",
      " -7.110330e-01  1.620320e-01  1.627490e-01  5.528180e-01  1.016860e-01\n",
      "  1.060080e-01  7.820700e-01 -7.537310e-01 -2.108400e-02 -4.758250e-01\n",
      " -1.130420e-01 -2.053000e-01  6.624390e-01  2.435850e-01  9.171890e-01\n",
      " -2.090610e-01 -5.290000e-02 -7.969340e-01  2.394940e-01 -9.028100e-02\n",
      "  1.537360e-01 -4.003980e-01 -2.456100e-02 -1.717860e-01  2.037790e-01\n",
      " -4.344710e-01 -3.850430e-01 -9.366000e-02  3.775310e-01  2.659690e-01\n",
      "  8.879800e-02  2.493440e-01  4.914900e-02  5.996000e-03  3.586430e-01\n",
      " -1.044960e-01 -5.838460e-01  3.093280e-01 -2.828090e-01 -8.563400e-02\n",
      " -5.745400e-02 -2.075230e-01  2.845980e-01  1.414760e-01  1.678570e-01\n",
      "  1.957560e-01  7.782140e-01 -2.359000e-01 -6.833100e-02  2.560170e-01\n",
      " -6.906900e-02 -1.219620e-01  2.683020e-01  1.678810e-01  2.068910e-01\n",
      "  1.987520e-01  6.720900e-02 -3.975290e-01 -7.123140e-01  5.613200e-02\n",
      "  2.586000e-03  5.616910e-01  1.157000e-03 -4.341190e-01  1.977480e-01\n",
      "  2.519540e-01  8.835000e-03 -3.554600e-01 -1.573500e-02 -2.526010e-01\n",
      "  9.355900e-02 -3.962500e-02 -1.628350e-01  2.980950e-01  1.647900e-01\n",
      " -5.454270e-01  3.888790e-01  1.446840e-01 -7.239600e-02 -7.597800e-02\n",
      " -7.803000e-03  2.020520e-01 -4.424750e-01  3.911580e-01  2.115100e-01\n",
      "  6.516760e-01  5.668030e-01  5.065500e-02 -1.259650e-01 -3.720640e-01\n",
      "  2.330470e-01  6.659900e-02  8.300600e-02  2.540460e-01 -5.279760e-01\n",
      " -3.843280e-01  3.366460e-01  2.336500e-01  3.564750e-01 -4.884160e-01\n",
      " -1.183910e-01  1.365910e-01  2.293420e-01 -6.151930e-01  5.212050e-01\n",
      "  3.412000e-01  5.757940e-01  2.354480e-01 -3.641530e-01  7.373400e-02\n",
      "  1.007380e-01 -3.211410e-01 -3.040480e-01 -3.738440e-01 -2.515150e-01\n",
      "  2.633890e-01  3.995490e-01  4.461880e-01  1.641110e-01  1.449590e-01\n",
      " -4.191540e-01  2.297840e-01  6.710600e-02  3.316430e-01 -6.026500e-02\n",
      " -5.130610e-01  1.472570e-01  2.414060e-01  2.011000e-03 -3.823410e-01\n",
      " -1.356010e-01  3.112300e-01  9.177830e-01 -4.511630e-01  1.272190e-01\n",
      " -9.431600e-02 -8.216000e-03 -3.835440e-01  2.589400e-02  6.374980e-01\n",
      "  4.931630e-01 -1.865070e-01  4.076900e-01 -1.841000e-03  2.213160e-01\n",
      "  2.253950e-01 -2.159220e-01 -7.611480e-01 -2.305920e-01  1.296890e-01\n",
      " -1.304100e-01 -4.742270e-01  2.275500e-02  4.255050e-01  1.570280e-01\n",
      "  2.975300e-02  1.931830e-01  1.304340e-01 -3.179800e-02  1.516650e-01\n",
      " -2.154310e-01 -4.681410e-01  1.007326e+00 -6.698940e-01 -1.555240e-01\n",
      "  1.797170e-01  2.848660e-01  6.216130e-01  1.549510e-01  6.225000e-02\n",
      " -2.227800e-02  2.561270e-01 -1.006380e-01  2.807900e-02  4.597710e-01\n",
      " -4.077750e-01 -1.777390e-01  1.920500e-02 -4.829300e-02  4.714700e-02\n",
      " -3.715200e-01 -2.995930e-01 -3.719710e-01  4.622800e-02 -1.436460e-01\n",
      "  2.532540e-01 -9.334000e-02 -4.957400e-02 -3.803850e-01  5.970110e-01\n",
      "  3.578450e-01 -6.826000e-02  4.735200e-02 -3.707590e-01 -8.621300e-02\n",
      " -2.556480e-01 -5.950440e-01 -4.757790e-01  1.079320e-01  9.858300e-02\n",
      "  8.540300e-01  3.518370e-01 -1.306360e-01 -1.541590e-01  1.166775e+00\n",
      "  2.048860e-01  5.952340e-01  1.158830e-01  6.774400e-02  6.793920e-01\n",
      " -3.610700e-01  1.697870e-01  4.118530e-01  4.731000e-03 -7.516530e-01\n",
      " -9.833700e-02 -2.312220e-01 -7.043300e-02  1.576110e-01 -4.780500e-02\n",
      " -7.344390e-01 -2.834330e-01  4.582690e-01  3.957010e-01 -8.484300e-02\n",
      " -3.472550e-01  1.291660e-01  3.838960e-01 -3.287600e-02 -2.802220e-01\n",
      "  5.257030e-01 -3.609300e-02 -4.842220e-01  3.690700e-02  3.429560e-01\n",
      "  2.902490e-01 -1.624650e-01 -7.513700e-02  2.669300e-01  5.778230e-01\n",
      " -3.074020e-01 -2.183790e-01 -2.834050e-01  1.350870e-01  1.490070e-01\n",
      "  1.438400e-02 -2.509040e-01 -3.376100e-01  1.291880e-01 -3.808700e-01\n",
      " -4.420520e-01 -2.512300e-01 -1.328990e-01 -1.211970e-01  2.532660e-01\n",
      "  2.757050e-01 -3.382040e-01  1.178070e-01  3.860190e-01  5.277960e-01\n",
      "  4.581920e-01  1.502310e-01  1.226320e-01  2.768540e-01 -4.502080e-01\n",
      " -1.992670e-01  1.689100e-02  1.188860e-01  3.502440e-01 -4.064770e-01\n",
      "  2.610280e-01 -1.934990e-01 -1.625660e-01  2.498400e-02 -1.867150e-01\n",
      " -1.954400e-02 -2.281900e-01 -3.417670e-01 -5.222770e-01 -9.543200e-02\n",
      " -3.500350e-01  2.154600e-02  2.318040e-01  5.395310e-01 -4.223720e-01]\n",
      "0.66128117\n",
      "sim:  0.66128117\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = cn_model['山东大学'].shape[0]  #一词山东大学，shape[0]返回行数\n",
    "print('词向量的长度为{}'.format(embedding_dim))\n",
    "print(cn_model['山东大学'])\n",
    "\n",
    "# 计算相似度\n",
    "print(cn_model.similarity('橘子', '橙子'))\n",
    "\n",
    "# dot（'橘子'/|'橘子'|， '橙子'/|'橙子'| ），余弦相似度\n",
    "sim = np.dot(cn_model['橘子']/np.linalg.norm(cn_model['橘子']),\n",
    "        cn_model['橙子']/np.linalg.norm(cn_model['橙子']))\n",
    "print(\"sim: \", sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('高中', 0.7247823476791382), ('本科', 0.6768535375595093), ('研究生', 0.6244412660598755), ('中学', 0.6088204979896545), ('大学本科', 0.595908522605896), ('初中', 0.5883588790893555), ('读研', 0.5778335332870483), ('职高', 0.5767995119094849), ('大学毕业', 0.5767451524734497), ('师范大学', 0.5708829760551453)]\n"
     ]
    }
   ],
   "source": [
    "# 找出最相近的词，余弦相似度\n",
    "print(cn_model.most_similar(positive=['大学'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 老师 会计师 程序员 律师 医生 老人 中:\n",
      "不是同一类别的词为: 老人\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "# 找出不同的词\n",
    "test_words = '老师 会计师 程序员 律师 医生 老人'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('在 '+test_words+' 中:\\n不是同一类别的词为: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('劈腿', 0.5849199295043945)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['女人','出轨'], negative=['男人'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总共: 4000\n"
     ]
    }
   ],
   "source": [
    "# 训练语料 （数据集）\n",
    "# 本教程使用了酒店评论语料，训练样本分别被放置在两个文件夹里： 分别的pos和neg，\n",
    "# 每个文件夹里有2000个txt文件，每个文件内有一段评语，共有4000个训练样本，这样大小的样本数据在NLP中属于非常迷你的\n",
    "\n",
    "# 获得样本的索引，样本存放于两个文件夹中，\n",
    "# 分别为 正面评价'pos'文件夹 和 负面评价'neg'文件夹\n",
    "# 每个文件夹中有2000个txt文件，每个文件中是一例评价，一个对一个\n",
    "import os\n",
    "pos_txts = os.listdir('data/pos')\n",
    "neg_txts = os.listdir('data/neg')\n",
    "print( '样本总共: '+ str(len(pos_txts) + len(neg_txts)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将所有的评价内容放置到一个list里\n",
    "train_texts_orig = [] # 存储所有评价，每例评价为一条string，原始评论\n",
    "# 添加完所有样本之后，train_texts_orig为一个含有4000条文本的list\n",
    "# 其中前2000条文本为正面评价，后2000条为负面评价\n",
    "#以下为读入.txt文件过程\n",
    "for i in range(len(pos_txts)):\n",
    "    with open('data/pos/'+pos_txts[i], 'r', errors='ignore',encoding='gbk') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "for i in range(len(neg_txts)):\n",
    "    with open('data/neg/'+neg_txts[i], 'r', errors='ignore',encoding='gbk') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['非常好的服务，不愧是四星以及Marrito连锁酒店。服务很专业',\n",
       " '这次在51期间通过电话订的2间豪华单间，到了一看，豪华单间就象套房一样，还是一室一厅的，位于22楼，视野开阔，客厅还放了很多介绍洛阳的旅游书。\\n\\n酒店地理位置也可以，离龙门石窟只有12公里，离市中心1公里。不足就是酒店设施稍旧。',\n",
       " '我在该宾馆入住了大约有20几次了吧，主要是觉得位置不错，交通方便。觉得难以忍受的是床垫实在是太太太硬了。',\n",
       " '酒店地理位置在古城区最繁华的十全街上，环境非常好，很有园林特色，让人感到很舒服很惬意！前台入住和退房的速度也很快，服务人员很友善，有问必答！房间也很有特色，墙上刻有金色的诗词，听说每个房间的诗词都不同呢！下次还会选择这里！',\n",
       " '很喜欢，而且解放北路也很漂亮，住这里晚上出来散步很好']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_orig[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.318 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[175, 72, 1, 1191, 13497, 4, 0, 273, 0, 12778, 1845, 1191, 34, 413],\n",
       " [817,\n",
       "  15,\n",
       "  8305,\n",
       "  975,\n",
       "  259,\n",
       "  927,\n",
       "  6247,\n",
       "  1,\n",
       "  278,\n",
       "  941,\n",
       "  8707,\n",
       "  25453,\n",
       "  48,\n",
       "  3,\n",
       "  0,\n",
       "  8707,\n",
       "  25453,\n",
       "  0,\n",
       "  9050,\n",
       "  134,\n",
       "  57,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  3056,\n",
       "  2734,\n",
       "  1425,\n",
       "  4416,\n",
       "  10236,\n",
       "  4265,\n",
       "  0,\n",
       "  3,\n",
       "  87,\n",
       "  1113,\n",
       "  8693,\n",
       "  1,\n",
       "  1588,\n",
       "  473,\n",
       "  1845,\n",
       "  11670,\n",
       "  18,\n",
       "  35,\n",
       "  1027,\n",
       "  171950,\n",
       "  186,\n",
       "  668,\n",
       "  1149,\n",
       "  1027,\n",
       "  9724,\n",
       "  280,\n",
       "  1149,\n",
       "  1803,\n",
       "  25,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.42575"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引长度标准化\n",
    "# 因为每段评语的长度是不一样的，我们如果单纯取最长的一个评语，并把其他评填充成同样的长度，\n",
    "# 这样十分浪费计算资源，所以我们取一个折衷的长度。\n",
    "\n",
    "# 获得所有tokens的长度\n",
    "num_tokens = [len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 平均tokens的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(num_tokens), bins = 100)#有大有小取对数\n",
    "plt.xlim((0,20))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9565\n"
     ]
    }
   ],
   "source": [
    "# 取tokens的长度为236时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "print(np.sum(num_tokens < max_tokens) / len(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向tokenize\n",
    "# 为了之后来验证 我们定义一个function，用来把索引转换成可阅读的文本，这对于debug很重要。\n",
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这次在51期间通过电话订的2间豪华单间到了 豪华单间 套房一样还是  的位于22楼视野开阔客厅 了很多介绍洛阳的旅游书酒店地理位置也可以离龙门石窟只有12公里离市中心1公里不足就是  '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这次在51期间通过电话订的2间豪华单间，到了一看，豪华单间就象套房一样，还是一室一厅的，位于22楼，视野开阔，客厅还放了很多介绍洛阳的旅游书。\\n\\n酒店地理位置也可以，离龙门石窟只有12公里，离市中心1公里。不足就是酒店设施稍旧。'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_orig[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'的'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.index2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建embedding matrix\n",
    "# 现在我们来为模型准备embedding matrix（词向量矩阵），根据keras的要求，我们需要准备一个维度为(numwords, embeddingdim)的矩阵\n",
    "# 【num words代表我们使用的词汇的数量，emdedding dimension在我们现在使用的预训练词向量模型中是300，\n",
    "# 每一个词汇都用一个长度为300的向量表示】注意我们只选择使用前50k个使用频率最高的词，在这个预训练词向量模型中，\n",
    "# 一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为我们的训练样本很小，一共只有4k，如果我们有100k，\n",
    "# 200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。\n",
    "\n",
    "# 只使用大库前50000个词\n",
    "num_words = 50000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.017840e-01, -1.653400e-01,  3.050800e-02, ...,  1.065250e-01,\n",
       "         5.534360e-01,  4.366500e-01],\n",
       "       [-6.517470e-01,  5.359700e-01,  3.402710e-01, ...,  8.053990e-01,\n",
       "         1.045930e-01,  1.936940e-01],\n",
       "       [-4.123210e-01,  2.282610e-01,  2.071140e-01, ...,  8.087770e-01,\n",
       "         5.675100e-02,  4.523740e-01],\n",
       "       ...,\n",
       "       [ 5.849840e-01,  1.121180e-01, -6.938330e-01, ..., -3.760570e-01,\n",
       "         1.203500e-01, -1.059511e+00],\n",
       "       [ 1.511710e-01, -3.200000e-04, -3.885760e-01, ..., -5.988550e-01,\n",
       "         4.273530e-01, -3.922630e-01],\n",
       "       [-4.536090e-01, -1.813600e-02, -1.306600e-01, ..., -6.608000e-02,\n",
       "         3.566680e-01,  3.898050e-01]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0  8399    57   562     1   312  2162 18472     0    73     1\n",
      "   221    57   562     1     0  1191    18   562]\n"
     ]
    }
   ],
   "source": [
    "# padding(填充)和truncating(修剪)\n",
    "# 我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，\n",
    "# 上面我们选择了236这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用’pre’的方法，\n",
    "# 这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。\n",
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "\n",
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[ train_pad>=num_words ] = 0\n",
    "\n",
    "\n",
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "print(train_pad[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target向量，前2000样本为1，后2000为0\n",
    "train_target = np.concatenate( (np.ones(2000),np.zeros(2000)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 236)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "#因为前2000个文件夹都是neg一类，所以打乱顺序来训练 random_state\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                              本人是比较喜欢这家酒店的已是 入住洲际位置很好就在维多利亚海星光大道旁边房间和酒店的大堂 都是落地大玻璃窗窗外美景 特别到晚上更甚就是房价不是个 携程这次订的是 2400不含早餐如果不 的话要3500这样一个晚上我曾经住过如果说不在乎银子的应该这里是上上 \n",
      "class:  1.0\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[35]))\n",
    "print('class: ',y_train[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用keras搭建LSTM模型\n",
    "# 模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。\n",
    "# keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。在Embedding层我们输入的矩阵为：(batchsize, maxtokens)，\n",
    "# 输出矩阵为：(batchsize, maxtokens, embeddingdim)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional#Dense全连接\n",
    "#Bidirectional双向LSTM  callbacks用来调参\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型第一层为embedding，trainable=False因为embedding_matrix下载后已经训练好了\n",
    "model.add(\n",
    "    Embedding(\n",
    "        num_words, embedding_dim , weights=[embedding_matrix], input_length=max_tokens, trainable=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(units=32, return_sequences=True)))#双向LSTM考虑前后词\n",
    "model.add(LSTM(units=16, return_sequences=False))#units=16神经元个数\n",
    "# 测试了LSTM和BiLSTM，发现BiLSTM的表现最好，LSTM的表现略好于GRU，因为BiLSTM对于比较长的句子结构有更好的记忆。\n",
    "# Embedding之后第，一层我们用BiLSTM返回sequences，然后第二层16个单元的LSTM不返回sequences，只返回最终结果，最后是一个全链接层，用sigmoid激活函数输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加入全连接层\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,  \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 236, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 236, 64)           85248     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16)                5184      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,090,449\n",
      "Trainable params: 90,449\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 我们来看一下模型的结构，一共90k左右可训练的变量，None表示batchsize,一个batch有236词输入\n",
    "#15000000为50000*300，因为train=false,所以不训练这些参数\n",
    "#17=16*1+1(bias为一参数)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点，verbose=1可以是打印信息更加详细，方面查找问题\n",
    "path_checkpoint = '/root/recmd_proj/data_nlp/sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.ModelCheckpoint at 0x7f9439836dd8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to open file (unable to open file: name = '/root/recmd_proj/data_nlp/sentiment_checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-5, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3240 samples, validate on 360 samples\n",
      "Epoch 1/4\n",
      "3240/3240 [==============================] - 25s 8ms/step - loss: 0.6231 - accuracy: 0.6577 - val_loss: 0.5795 - val_accuracy: 0.7056\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57950, saving model to /root/recmd_proj/data_nlp/sentiment_checkpoint.keras\n",
      "Epoch 2/4\n",
      "3240/3240 [==============================] - 22s 7ms/step - loss: 0.4561 - accuracy: 0.8012 - val_loss: 0.4877 - val_accuracy: 0.7694\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57950 to 0.48765, saving model to /root/recmd_proj/data_nlp/sentiment_checkpoint.keras\n",
      "Epoch 3/4\n",
      "3240/3240 [==============================] - 22s 7ms/step - loss: 0.3779 - accuracy: 0.8429 - val_loss: 0.4049 - val_accuracy: 0.8361\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48765 to 0.40491, saving model to /root/recmd_proj/data_nlp/sentiment_checkpoint.keras\n",
      "Epoch 4/4\n",
      "3240/3240 [==============================] - 22s 7ms/step - loss: 0.3536 - accuracy: 0.8556 - val_loss: 0.4319 - val_accuracy: 0.8083\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.40491\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f941b4594e0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练，4000*0.1=400为test，validation_split=0.1为3600*0.1\n",
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=4,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 4ms/step\n",
      "Accuracy:83.25%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们定义一个预测函数（将输入文本按模型要求处理再输入），来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断\n",
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是一例正面评价','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是一例负面评价','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店设施不是新的，服务态度不好\n",
      "是一例负面评价 output=0.17\n",
      "房间很凉爽，空调冷气很足\n",
      "是一例负面评价 output=0.50\n",
      "酒店环境不好，住宿体验很不好\n",
      "是一例负面评价 output=0.10\n",
      "房间隔音不到位\n",
      "是一例负面评价 output=0.28\n",
      "晚上回来发现没有打扫卫生\n",
      "是一例负面评价 output=0.30\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '酒店设施不是新的，服务态度不好',\n",
    "    '房间很凉爽，空调冷气很足',\n",
    "    '酒店环境不好，住宿体验很不好',\n",
    "    '房间隔音不到位' ,\n",
    "    '晚上回来发现没有打扫卫生'   \n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搜狐新闻文本分类(word2vec）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>《青蛇》造型师默认新《红楼梦》额妆抄袭（图） 凡是看过电影《青蛇》的人，都不会忘记青白二蛇的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>６．１６日剧榜　＜最后的朋友＞　亮最后杀招成功登顶 《最后的朋友》本周的电视剧排行榜单依然只...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>超乎想象的好看《纳尼亚传奇２：凯斯宾王子》 现时资讯如此发达，搜狐电影评审团几乎人人在没有看...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>吴宇森：赤壁大战不会出现在上集 “希望《赤壁》能给你们不一样的感觉。”对于自己刚刚拍完的影片...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>组图：《多情女人痴情男》陈浩民现场耍宝 陈浩民：外面的朋友大家好，现在是搜狐现场直播，欢迎《...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                                                  1\n",
       "0  娱乐  《青蛇》造型师默认新《红楼梦》额妆抄袭（图） 凡是看过电影《青蛇》的人，都不会忘记青白二蛇的...\n",
       "1  娱乐  ６．１６日剧榜　＜最后的朋友＞　亮最后杀招成功登顶 《最后的朋友》本周的电视剧排行榜单依然只...\n",
       "2  娱乐  超乎想象的好看《纳尼亚传奇２：凯斯宾王子》 现时资讯如此发达，搜狐电影评审团几乎人人在没有看...\n",
       "3  娱乐  吴宇森：赤壁大战不会出现在上集 “希望《赤壁》能给你们不一样的感觉。”对于自己刚刚拍完的影片...\n",
       "4  娱乐  组图：《多情女人痴情男》陈浩民现场耍宝 陈浩民：外面的朋友大家好，现在是搜狐现场直播，欢迎《..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练集数据共有24000条，测试集数据共有12000条\n",
    "import pandas as pd\n",
    "#加载训练集到变量train_df中，并打印训练集前5行，代码如下。\n",
    "#read_csv方法中有3个参数，第1个参数是加载文本文件的路径，第2个关键字参数sep是分隔符，第3个关键字参数header是文本文件的第1行是否为字段名。\n",
    "train_df = pd.read_csv('./data/sohu_train.txt', sep='\\t', header=None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>房地产</td>\n",
       "      <td>业内论坛 简单。新总统得和布什切割，从海湾撤军，美国财力恢复，国内进口消费转为景气，中国出口...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19455</th>\n",
       "      <td>新闻</td>\n",
       "      <td>改革开放３０年 来源：中华英才半月刊徐友渔我们即将迎来改革开放３０年。这３０年，虽说是“摸着...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>房地产</td>\n",
       "      <td>等了７年还没拿到　办个房产证怎就这么难？ －－广州４５万宗房产证“难产”－－开发商欠交地价款...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5344</th>\n",
       "      <td>房地产</td>\n",
       "      <td>广州住宅郊区化发展趋势探析 郊区住宅的优势及实现住宅郊区化应具备的条件居住郊区是发达国家，也...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>辽宁有线二　　０７：４０　新闻视线（周二．周四．周六） １１月２７日－１２月０３日ＣＣＴＶ６...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12976</th>\n",
       "      <td>健康</td>\n",
       "      <td>家用电器辐射数据大揭秘　７个秘籍巧防辐射 长时间使用电热毯睡觉的女性，可使月经周期发生明显改...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10636</th>\n",
       "      <td>体育</td>\n",
       "      <td>女排一场硬仗显强队本色　两大收获重塑陈家军魂 两强相争勇者胜。昨晚，中国女排苦战五局以３∶２...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>陈小春谈对婚姻的困惑　坚称绝对不能戴绿帽子 “古惑仔”陈小春。许久没演电影，陈小春将在银幕上...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22561</th>\n",
       "      <td>女人</td>\n",
       "      <td>７０、８０、９０后掐到何时休？ “９０后贱女孩”包包和阿紫比之芙蓉姐姐、本田雅阁女、山东二哥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>财经</td>\n",
       "      <td>【编者按】：原本“守得云开见月明”的四大资产管理公司（ＡＭＣ），忽然发现未来的道路并不平坦...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                                  1\n",
       "5919   房地产  业内论坛 简单。新总统得和布什切割，从海湾撤军，美国财力恢复，国内进口消费转为景气，中国出口...\n",
       "19455   新闻  改革开放３０年 来源：中华英才半月刊徐友渔我们即将迎来改革开放３０年。这３０年，虽说是“摸着...\n",
       "5904   房地产  等了７年还没拿到　办个房产证怎就这么难？ －－广州４５万宗房产证“难产”－－开发商欠交地价款...\n",
       "5344   房地产  广州住宅郊区化发展趋势探析 郊区住宅的优势及实现住宅郊区化应具备的条件居住郊区是发达国家，也...\n",
       "1855    娱乐  辽宁有线二　　０７：４０　新闻视线（周二．周四．周六） １１月２７日－１２月０３日ＣＣＴＶ６...\n",
       "12976   健康  家用电器辐射数据大揭秘　７个秘籍巧防辐射 长时间使用电热毯睡觉的女性，可使月经周期发生明显改...\n",
       "10636   体育  女排一场硬仗显强队本色　两大收获重塑陈家军魂 两强相争勇者胜。昨晚，中国女排苦战五局以３∶２...\n",
       "1654    娱乐  陈小春谈对婚姻的困惑　坚称绝对不能戴绿帽子 “古惑仔”陈小春。许久没演电影，陈小春将在银幕上...\n",
       "22561   女人  ７０、８０、９０后掐到何时休？ “９０后贱女孩”包包和阿紫比之芙蓉姐姐、本田雅阁女、山东二哥...\n",
       "3661    财经   【编者按】：原本“守得云开见月明”的四大资产管理公司（ＡＭＣ），忽然发现未来的道路并不平坦..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育 2000\n",
      "健康 2000\n",
      "女人 2000\n",
      "娱乐 2000\n",
      "房地产 2000\n",
      "教育 2000\n",
      "文化 2000\n",
      "新闻 2000\n",
      "旅游 2000\n",
      "汽车 2000\n",
      "科技 2000\n",
      "财经 2000\n"
     ]
    }
   ],
   "source": [
    "#查看训练集每个分类的名字以及样本数量\n",
    "for name,group in train_df.groupby(0):\n",
    "    print(name,len(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育 1000\n",
      "健康 1000\n",
      "女人 1000\n",
      "娱乐 1000\n",
      "房地产 1000\n",
      "教育 1000\n",
      "文化 1000\n",
      "新闻 1000\n",
      "旅游 1000\n",
      "汽车 1000\n",
      "科技 1000\n",
      "财经 1000\n"
     ]
    }
   ],
   "source": [
    "#加载测试集并查看每个分类的名字以及样本数量\n",
    "test_df = pd.read_csv('data/sohu_test.txt', sep='\\t', header=None)\n",
    "for name, group in test_df.groupby(0):\n",
    "    print(name, len(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前1000篇文章分词共花费43.49秒\n",
      "前2000篇文章分词共花费87.62秒\n",
      "前3000篇文章分词共花费326.03秒\n",
      "前4000篇文章分词共花费477.73秒\n",
      "前5000篇文章分词共花费666.60秒\n",
      "前6000篇文章分词共花费949.10秒\n"
     ]
    }
   ],
   "source": [
    "#对训练集的24000条样本循环遍历，使用jieba库的cut方法获得分词列表赋值给变量cutWords\n",
    "#判断分词是否为停顿词，如果不为停顿词，则添加进变量cutWords中\n",
    "import jieba\n",
    "import time\n",
    "\n",
    "train_df.columns = ['分类', '文章']\n",
    "stopword_list = [k.strip() for k in open('data/stopwords.txt', encoding='utf8').readlines() if k.strip() != '']\n",
    "cutWords_list = []\n",
    "i = 0\n",
    "startTime = time.time()\n",
    "for article in train_df['文章']:\n",
    "    cutWords = [k for k in jieba.cut(article) if k not in stopword_list]\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print('前%d篇文章分词共花费%.2f秒' %(i, time.time()-startTime))\n",
    "    cutWords_list.append(cutWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用gensim.models.word2vec库中的LineSentence方法实例化模型对象\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(cutWords_list, size=100, iter=10, min_count=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
