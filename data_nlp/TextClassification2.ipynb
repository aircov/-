{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "#KeyedVectors实现实体（单词、文档、图片都可以）和向量之间的映射，实体都用string id表示\n",
    "#有时候运行代码时会有很多warning输出，如提醒新版本之类的，如果不想乱糟糟的输出可以这样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练词向量\n",
    "# 使用gensim加载预训练中文分词embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('data/sgns.zhihu.bigram',\n",
    "                                          binary=False)\n",
    "# 词向量模型\n",
    "# 在这个词向量模型里，每一个词是一个索引，对应的是一个长度为300的向量，我们今天需要构建的LSTM神经网络模型并不能直接处理汉字文本，\n",
    "# 需要先进行分次并把词汇转换为词向量，步骤请参考：\n",
    "# 0.原始文本：我喜欢文学\n",
    "# 1.分词：我，喜欢，文学\n",
    "# 2.Tokenize(索引化)：[2,345，4564]\n",
    "# 3.Embedding(词向量化)：用一个300维的词向量，上面的tokens成为一个[3，300]的矩阵\n",
    "# 4.RNN:1DCONV,GRU,LSTM等\n",
    "# 5.经过激活函数输出分类：如sigmoid输出在0到1间\n",
    "# 由此可见每一个词都对应一个长度为300的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的长度为300\n",
      "[-2.603470e-01  3.677500e-01 -2.379650e-01  5.301700e-02 -3.628220e-01\n",
      " -3.212010e-01 -1.903330e-01  1.587220e-01 -7.156200e-02 -4.625400e-02\n",
      " -1.137860e-01  3.515600e-01 -6.408200e-02 -2.184840e-01  3.286950e-01\n",
      " -7.110330e-01  1.620320e-01  1.627490e-01  5.528180e-01  1.016860e-01\n",
      "  1.060080e-01  7.820700e-01 -7.537310e-01 -2.108400e-02 -4.758250e-01\n",
      " -1.130420e-01 -2.053000e-01  6.624390e-01  2.435850e-01  9.171890e-01\n",
      " -2.090610e-01 -5.290000e-02 -7.969340e-01  2.394940e-01 -9.028100e-02\n",
      "  1.537360e-01 -4.003980e-01 -2.456100e-02 -1.717860e-01  2.037790e-01\n",
      " -4.344710e-01 -3.850430e-01 -9.366000e-02  3.775310e-01  2.659690e-01\n",
      "  8.879800e-02  2.493440e-01  4.914900e-02  5.996000e-03  3.586430e-01\n",
      " -1.044960e-01 -5.838460e-01  3.093280e-01 -2.828090e-01 -8.563400e-02\n",
      " -5.745400e-02 -2.075230e-01  2.845980e-01  1.414760e-01  1.678570e-01\n",
      "  1.957560e-01  7.782140e-01 -2.359000e-01 -6.833100e-02  2.560170e-01\n",
      " -6.906900e-02 -1.219620e-01  2.683020e-01  1.678810e-01  2.068910e-01\n",
      "  1.987520e-01  6.720900e-02 -3.975290e-01 -7.123140e-01  5.613200e-02\n",
      "  2.586000e-03  5.616910e-01  1.157000e-03 -4.341190e-01  1.977480e-01\n",
      "  2.519540e-01  8.835000e-03 -3.554600e-01 -1.573500e-02 -2.526010e-01\n",
      "  9.355900e-02 -3.962500e-02 -1.628350e-01  2.980950e-01  1.647900e-01\n",
      " -5.454270e-01  3.888790e-01  1.446840e-01 -7.239600e-02 -7.597800e-02\n",
      " -7.803000e-03  2.020520e-01 -4.424750e-01  3.911580e-01  2.115100e-01\n",
      "  6.516760e-01  5.668030e-01  5.065500e-02 -1.259650e-01 -3.720640e-01\n",
      "  2.330470e-01  6.659900e-02  8.300600e-02  2.540460e-01 -5.279760e-01\n",
      " -3.843280e-01  3.366460e-01  2.336500e-01  3.564750e-01 -4.884160e-01\n",
      " -1.183910e-01  1.365910e-01  2.293420e-01 -6.151930e-01  5.212050e-01\n",
      "  3.412000e-01  5.757940e-01  2.354480e-01 -3.641530e-01  7.373400e-02\n",
      "  1.007380e-01 -3.211410e-01 -3.040480e-01 -3.738440e-01 -2.515150e-01\n",
      "  2.633890e-01  3.995490e-01  4.461880e-01  1.641110e-01  1.449590e-01\n",
      " -4.191540e-01  2.297840e-01  6.710600e-02  3.316430e-01 -6.026500e-02\n",
      " -5.130610e-01  1.472570e-01  2.414060e-01  2.011000e-03 -3.823410e-01\n",
      " -1.356010e-01  3.112300e-01  9.177830e-01 -4.511630e-01  1.272190e-01\n",
      " -9.431600e-02 -8.216000e-03 -3.835440e-01  2.589400e-02  6.374980e-01\n",
      "  4.931630e-01 -1.865070e-01  4.076900e-01 -1.841000e-03  2.213160e-01\n",
      "  2.253950e-01 -2.159220e-01 -7.611480e-01 -2.305920e-01  1.296890e-01\n",
      " -1.304100e-01 -4.742270e-01  2.275500e-02  4.255050e-01  1.570280e-01\n",
      "  2.975300e-02  1.931830e-01  1.304340e-01 -3.179800e-02  1.516650e-01\n",
      " -2.154310e-01 -4.681410e-01  1.007326e+00 -6.698940e-01 -1.555240e-01\n",
      "  1.797170e-01  2.848660e-01  6.216130e-01  1.549510e-01  6.225000e-02\n",
      " -2.227800e-02  2.561270e-01 -1.006380e-01  2.807900e-02  4.597710e-01\n",
      " -4.077750e-01 -1.777390e-01  1.920500e-02 -4.829300e-02  4.714700e-02\n",
      " -3.715200e-01 -2.995930e-01 -3.719710e-01  4.622800e-02 -1.436460e-01\n",
      "  2.532540e-01 -9.334000e-02 -4.957400e-02 -3.803850e-01  5.970110e-01\n",
      "  3.578450e-01 -6.826000e-02  4.735200e-02 -3.707590e-01 -8.621300e-02\n",
      " -2.556480e-01 -5.950440e-01 -4.757790e-01  1.079320e-01  9.858300e-02\n",
      "  8.540300e-01  3.518370e-01 -1.306360e-01 -1.541590e-01  1.166775e+00\n",
      "  2.048860e-01  5.952340e-01  1.158830e-01  6.774400e-02  6.793920e-01\n",
      " -3.610700e-01  1.697870e-01  4.118530e-01  4.731000e-03 -7.516530e-01\n",
      " -9.833700e-02 -2.312220e-01 -7.043300e-02  1.576110e-01 -4.780500e-02\n",
      " -7.344390e-01 -2.834330e-01  4.582690e-01  3.957010e-01 -8.484300e-02\n",
      " -3.472550e-01  1.291660e-01  3.838960e-01 -3.287600e-02 -2.802220e-01\n",
      "  5.257030e-01 -3.609300e-02 -4.842220e-01  3.690700e-02  3.429560e-01\n",
      "  2.902490e-01 -1.624650e-01 -7.513700e-02  2.669300e-01  5.778230e-01\n",
      " -3.074020e-01 -2.183790e-01 -2.834050e-01  1.350870e-01  1.490070e-01\n",
      "  1.438400e-02 -2.509040e-01 -3.376100e-01  1.291880e-01 -3.808700e-01\n",
      " -4.420520e-01 -2.512300e-01 -1.328990e-01 -1.211970e-01  2.532660e-01\n",
      "  2.757050e-01 -3.382040e-01  1.178070e-01  3.860190e-01  5.277960e-01\n",
      "  4.581920e-01  1.502310e-01  1.226320e-01  2.768540e-01 -4.502080e-01\n",
      " -1.992670e-01  1.689100e-02  1.188860e-01  3.502440e-01 -4.064770e-01\n",
      "  2.610280e-01 -1.934990e-01 -1.625660e-01  2.498400e-02 -1.867150e-01\n",
      " -1.954400e-02 -2.281900e-01 -3.417670e-01 -5.222770e-01 -9.543200e-02\n",
      " -3.500350e-01  2.154600e-02  2.318040e-01  5.395310e-01 -4.223720e-01]\n",
      "0.66128117\n",
      "sim:  0.6612812\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = cn_model['山东大学'].shape[0]  #一词山东大学，shape[0]返回行数\n",
    "print('词向量的长度为{}'.format(embedding_dim))\n",
    "print(cn_model['山东大学'])\n",
    "\n",
    "# 计算相似度\n",
    "print(cn_model.similarity('橘子', '橙子'))\n",
    "\n",
    "# dot（'橘子'/|'橘子'|， '橙子'/|'橙子'| ），余弦相似度\n",
    "sim = np.dot(cn_model['橘子']/np.linalg.norm(cn_model['橘子']),\n",
    "        cn_model['橙子']/np.linalg.norm(cn_model['橙子']))\n",
    "print(\"sim: \", sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('高中', 0.7247821688652039), ('本科', 0.6768536567687988), ('研究生', 0.6244412660598755), ('中学', 0.6088206768035889), ('大学本科', 0.595908522605896), ('初中', 0.5883588790893555), ('读研', 0.5778335928916931), ('职高', 0.5767994523048401), ('大学毕业', 0.5767452120780945), ('师范大学', 0.5708829164505005)]\n"
     ]
    }
   ],
   "source": [
    "# 找出最相近的词，余弦相似度\n",
    "print(cn_model.most_similar(positive=['大学'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 老师 会计师 程序员 律师 医生 老人 中:\n",
      "不是同一类别的词为: 老人\n"
     ]
    }
   ],
   "source": [
    "# 找出不同的词\n",
    "test_words = '老师 会计师 程序员 律师 医生 老人'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('在 '+test_words+' 中:\\n不是同一类别的词为: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('劈腿', 0.5849197506904602)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['女人','出轨'], negative=['男人'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总共: 4000\n"
     ]
    }
   ],
   "source": [
    "# 训练语料 （数据集）\n",
    "# 本教程使用了酒店评论语料，训练样本分别被放置在两个文件夹里： 分别的pos和neg，\n",
    "# 每个文件夹里有2000个txt文件，每个文件内有一段评语，共有4000个训练样本，这样大小的样本数据在NLP中属于非常迷你的\n",
    "\n",
    "# 获得样本的索引，样本存放于两个文件夹中，\n",
    "# 分别为 正面评价'pos'文件夹 和 负面评价'neg'文件夹\n",
    "# 每个文件夹中有2000个txt文件，每个文件中是一例评价，一个对一个\n",
    "import os\n",
    "pos_txts = os.listdir('data/pos')\n",
    "neg_txts = os.listdir('data/neg')\n",
    "print( '样本总共: '+ str(len(pos_txts) + len(neg_txts)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将所有的评价内容放置到一个list里\n",
    "train_texts_orig = [] # 存储所有评价，每例评价为一条string，原始评论\n",
    "# 添加完所有样本之后，train_texts_orig为一个含有4000条文本的list\n",
    "# 其中前2000条文本为正面评价，后2000条为负面评价\n",
    "#以下为读入.txt文件过程\n",
    "for i in range(len(pos_txts)):\n",
    "    with open('data/pos/'+pos_txts[i], 'r', errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "for i in range(len(neg_txts)):\n",
    "    with open('data/neg/'+neg_txts[i], 'r', errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。\\n\\n房间本身很好。',\n",
       " '宾馆在小街道上，不大好找，但还好北京热心同胞很多~\\n\\n宾馆设施跟介绍的差不多，房间很小，确实挺小，但加上低价位因素，还是无超所值的；\\n\\n环境不错，就在小胡同内，安静整洁，暖气好足-_-||。。。呵\\n\\n还有一大优势就是从宾馆出发，步行不到十分钟就可以到梅兰芳故居等等，京味小胡同，北海距离好近呢。\\n\\n总之，不错。\\n\\n推荐给节约消费的自助游朋友~比较划算，附近特色小吃很多~',\n",
       " 'CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风',\n",
       " '总的来说，这样的酒店配这样的价格还算可以，希望他赶快装修，给我的客人留些好的印象',\n",
       " '价格比比较不错的酒店。这次免费升级了，感谢前台服务员。房子还好，地毯是新的，比上次的好些。早餐的人很多要早去些。']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_orig[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.329 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4656,\n",
       "  163,\n",
       "  710,\n",
       "  909,\n",
       "  32,\n",
       "  328,\n",
       "  12,\n",
       "  1899,\n",
       "  18,\n",
       "  8685,\n",
       "  1604,\n",
       "  1,\n",
       "  1845,\n",
       "  144,\n",
       "  2420,\n",
       "  153,\n",
       "  36,\n",
       "  75,\n",
       "  3,\n",
       "  1487,\n",
       "  571,\n",
       "  34,\n",
       "  72],\n",
       " [7224,\n",
       "  15,\n",
       "  95,\n",
       "  5976,\n",
       "  24,\n",
       "  31233,\n",
       "  173,\n",
       "  67,\n",
       "  2380,\n",
       "  497,\n",
       "  9074,\n",
       "  5139,\n",
       "  87,\n",
       "  7224,\n",
       "  5249,\n",
       "  103,\n",
       "  1113,\n",
       "  1,\n",
       "  869,\n",
       "  1487,\n",
       "  2871,\n",
       "  333,\n",
       "  470,\n",
       "  95,\n",
       "  67,\n",
       "  879,\n",
       "  83986,\n",
       "  1321,\n",
       "  57,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  110,\n",
       "  578,\n",
       "  562,\n",
       "  38,\n",
       "  15,\n",
       "  95,\n",
       "  9081,\n",
       "  457,\n",
       "  2567,\n",
       "  10199,\n",
       "  11216,\n",
       "  0,\n",
       "  148,\n",
       "  148,\n",
       "  2360,\n",
       "  2360,\n",
       "  7215,\n",
       "  158,\n",
       "  893,\n",
       "  1426,\n",
       "  38,\n",
       "  1353,\n",
       "  7224,\n",
       "  2289,\n",
       "  9265,\n",
       "  388,\n",
       "  0,\n",
       "  38,\n",
       "  35,\n",
       "  48,\n",
       "  44101,\n",
       "  33287,\n",
       "  810,\n",
       "  115835,\n",
       "  95,\n",
       "  9081,\n",
       "  10984,\n",
       "  1117,\n",
       "  0,\n",
       "  147,\n",
       "  3466,\n",
       "  562,\n",
       "  657,\n",
       "  51,\n",
       "  6781,\n",
       "  1622,\n",
       "  1,\n",
       "  62779,\n",
       "  211,\n",
       "  169,\n",
       "  9852,\n",
       "  1256,\n",
       "  88407,\n",
       "  87]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.4495"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引长度标准化\n",
    "# 因为每段评语的长度是不一样的，我们如果单纯取最长的一个评语，并把其他评填充成同样的长度，\n",
    "# 这样十分浪费计算资源，所以我们取一个折衷的长度。\n",
    "\n",
    "# 获得所有tokens的长度\n",
    "num_tokens = [len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 平均tokens的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(num_tokens), bins = 100)#有大有小取对数\n",
    "plt.xlim((0,20))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9565\n"
     ]
    }
   ],
   "source": [
    "# 取tokens的长度为236时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "print(np.sum(num_tokens < max_tokens) / len(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向tokenize\n",
    "# 为了之后来验证 我们定义一个function，用来把索引转换成可阅读的文本，这对于debug很重要。\n",
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差无论去多少人那边也不加食品的酒店应该重视一下这个问题了房间本身很好'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。\\n\\n房间本身很好。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_orig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'的'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.index2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建embedding matrix\n",
    "# 现在我们来为模型准备embedding matrix（词向量矩阵），根据keras的要求，我们需要准备一个维度为(numwords, embeddingdim)的矩阵\n",
    "# 【num words代表我们使用的词汇的数量，emdedding dimension在我们现在使用的预训练词向量模型中是300，\n",
    "# 每一个词汇都用一个长度为300的向量表示】注意我们只选择使用前50k个使用频率最高的词，在这个预训练词向量模型中，\n",
    "# 一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为我们的训练样本很小，一共只有4k，如果我们有100k，\n",
    "# 200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。\n",
    "\n",
    "# 只使用大库前50000个词\n",
    "num_words = 50000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.017840e-01, -1.653400e-01,  3.050800e-02, ...,  1.065250e-01,\n",
       "         5.534360e-01,  4.366500e-01],\n",
       "       [-6.517470e-01,  5.359700e-01,  3.402710e-01, ...,  8.053990e-01,\n",
       "         1.045930e-01,  1.936940e-01],\n",
       "       [-4.123210e-01,  2.282610e-01,  2.071140e-01, ...,  8.087770e-01,\n",
       "         5.675100e-02,  4.523740e-01],\n",
       "       ...,\n",
       "       [ 5.849840e-01,  1.121180e-01, -6.938330e-01, ..., -3.760570e-01,\n",
       "         1.203500e-01, -1.059511e+00],\n",
       "       [ 1.511710e-01, -3.200000e-04, -3.885760e-01, ..., -5.988550e-01,\n",
       "         4.273530e-01, -3.922630e-01],\n",
       "       [-4.536090e-01, -1.813600e-02, -1.306600e-01, ..., -6.608000e-02,\n",
       "         3.566680e-01,  3.898050e-01]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-bd5be106f918>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 进行padding和truncating， 输入的train_tokens是一个list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 返回的train_pad是一个numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
      "\u001b[1;32mc:\\software\\python\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\software\\python\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\software\\python\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\software\\python\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\software\\python\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\software\\python\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# padding(填充)和truncating(修剪)\n",
    "# 我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，\n",
    "# 上面我们选择了236这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用’pre’的方法，\n",
    "# 这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。\n",
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "\n",
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[ train_pad>=num_words ] = 0\n",
    "\n",
    "\n",
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "print(train_pad[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
